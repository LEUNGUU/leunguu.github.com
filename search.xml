<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Codis监控工具]]></title>
    <url>%2F2018%2F01%2F08%2Fcodis-watch%2F</url>
    <content type="text"><![CDATA[背景由于之前成功地做了一次线上Codis集群不停机迁移，并且之后稳定的运行了一段时间。部门内的运维弟兄们看到了Codis的好处，也纷纷在各自负责的业务中推广使用。个人感觉Codis集群确实要比直接的redis主从或者sentinel要灵活很多，其中的好处大家可以去github上面搜索codis，最新的版本是codis3.0。由于使用广泛，其中也出现了大大小小的问题。 问题来临了其实这里只讨论最大的一个问题，就是监控。其实我们已经有使用nagios对redis-server或者proxy的端口进行监控，告警速度也比较快。但是，就像我在回忆录里面说到的，能不能自我修复呢？由于缓存的特性，故障几秒钟就有可能丢失大量的数据。所以我开始思考监控并且治愈。官方开源里面有一个codis-ha的工具，是一个后台进程。它对集群内所有的server group进行监控，一旦发现group内的master出问题了，马上进行group内主从切换。这个工具确实可以实现治愈，但是后面还需要一些善后的工作。比方说挂掉的master要启动并且手动重新加到group里面等等。但是这个codis-ha只是“默默地“治愈集群，并没有告诉我它干了什么。同时，因为codis-server经常变动，需要经常改动nagios监控也着实不方便。这就促使我思考改进的办法。 思路有了通过思考，我想自己实现一个codis-ha。既可以治愈集群，又可以发送告警消息。官方的codis-ha是使用go语言编写的，但我熟悉的是python。于是在理解了官方的实现逻辑之后，我自己写了一个python版的codis-ha，并且是带有告警功能的。分享在github，欢迎大家使用并改进。这个代码做了点修改，默认是发送邮件告警，如果需要，也可以加上短信或者微信告警。 Github: pyCodis-ha]]></content>
      <categories>
        <category>缓存</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Codis不停机迁移]]></title>
    <url>%2F2018%2F01%2F06%2Fcodis-migrate%2F</url>
    <content type="text"><![CDATA[背景当时从别的部门接手了一套服务，里面使用了一套豌豆荚开源的Codis2.0版本的缓存集群。但是由于历史原因，现行的架构不是很合理。集群的dashboard，proxy以及codis-server都集中复用在主要几台主机上。其中共有四台主机，每台主机角色都有proxy，ZookeeperServer以及codis-server。并且每台机器存在6个主实例，对于生产系统，这样的结构存在极大的安全隐患。于是考虑做一次迁移，将角色分散开来，降低风险。由于是生产系统，本次迁移必须是以不影响线上服务为前提，下面来叙述基本过程。 迁移过程这里其实分成了两部分，一部分是组件迁移，另一部分是数据迁移。 组件迁移新的规划是3台codis-proxy和zookeeper（复用在一起），dashboard一台，codis-server数台。组件迁移的目标是将旧机器上的组件按规划迁移到新机器上。下面描述详细过程。 Step-1因为原来有4台zk，那么我们先新增3台，令集群保持奇数。做法是在原来4台zk的server列表里面添加进新的3台，同时新的3台zk的server列表也是预先写好了7个server。这时候选择原有的4台zk每次两台进行重启（测试过程中发现一次重启两台集群会比较稳定）。这里描述一下里面的原理。其实zk只要集群里面的节点超过半数是存活的，集群就是稳定的。那么被重启后的两台机器认为现在存活的是4台（新的3台没有启动），超过半数，集群稳定。 那么这时候我们再启动后面的3台，这时候zk集群就变成了7台。 Step-2这时候我们将新的3台codis-proxy上线(proxy里面的zk server列表是新的3台zk)，修改LVS入口调度策略，将流量引到新的proxy上。然后将旧的4台zk任选3台停止上面的zk进程(这里为了让集群没有停顿，最好选择非leader节点)。然后类似Step-1的操作，修改余下4台zk的server列表，从中剔除已停止的3台，并每次选两台进行重启，此时zk集群变成4个节点。 Step-3迅速将旧的dashboard停掉，启动新的dashboard。然后将旧的4台codis-proxy中的zk server列表变成新的3台zk，然后逐个重启。这时候就完成了90%的工作了。 Step-4停掉原来剩下的1台zk，并更新zk集群的server列表，每次任选两台进行重启。此时zk集群成功变成了3个节点，proxy此时有7个（原有的4个和新上的3个）并且其中的zk列表都是新的3个zk节点。 到这里，我们就完成了迁移的第一步：组件迁移。其实这里面步骤有点复杂，但是都是围绕着zk集群不能超过半数宕机以及平滑地将zk里的数据同步到新zk节点上。接下来我们继续下一步。 数据迁移当前是有四台codis-server，每台有6个6G的实例。每个实例都是一个组的主实例。现在的目的是将数据迁移到14台codis-server中，每台codis-server只有一个6G实例，其中数据的清除规则设置的是Allkeys-LRU。由于之前执行了auto-rebalance操作，全部都打散了，故迁移过程主要是首先扫描集群的slot，取出需要组的slot_id，然后进行迁移。细心的您可能会发现这个数据迁移前后的数据量不一致。是的，我们不只是做迁移，还清理了一些垃圾数据。 总结到此整个不停机迁移的过程结束。做完这个过程确实收益不少，其中最重要的我认为是八个字：认真准备，胆大心细。由于这次操作的缓存是生产环境的，一出现差错会直接影响线上服务。所以操作之前进行了各种测试，做好了风险评估。实施的时候严格按照施工文档操作，步步为营，如履薄冰。 参考https://github.com/CodisLabs/codishttps://cwiki.apache.org/confluence/display/ZOOKEEPER/Index]]></content>
      <categories>
        <category>缓存</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回忆录（web服务篇）]]></title>
    <url>%2F2018%2F01%2F05%2Fthinking-about-web%2F</url>
    <content type="text"><![CDATA[初识服务记起当时在a公司的时候，除了负责大数据集群的维护之外，还需要维护一两个web服务。我们团队分工比较合理，每一个业务都有AB角，虽然角色交叉重叠，但是条理清楚，分工明确。当时我就是其中两个业务的运维B角色( 其实是替补😄 )。老实说，到这时候，我才是刚开始接触web服务。 开始积累我是从代码更新开始熟悉所管理的整套业务的。2015年的我们是业界所说的脚本时代，代码更新全是执行脚本。代码放在灰度或者测试环境，从生产环境下线机器，同步代码到已下线机器，重启，检查日志，正常则上线并继续更新余下机器；报错则和研发一起查看问题，能解决则继续，不能则回滚并记录升级失败。当时有很长一段时间都是专门做这一项工作，而且都是半夜三更等大家都睡了的时候才开始（更新只能选晚上流量不大的时候）。这个过程虽然艰苦，但是也积累了不少经验。比如业务逻辑，系统架构，查找问题的方法论等等。除此之外，当时我们还经常会遇到故障。因为云计算供应商的服务不是很稳定，时不时都会出现网络故障，机器故障等（甚至我们直接使用的物理机也时不时出现磁盘只读）。每次故障我都能学到不少东西，更加明白”养兵千日 用兵一时”的道理。在平时就应该尽量把事情考虑彻底，时刻准备着，才能更好应对突发情况。 开始思考代码更新令我熟悉整个服务管理流程。这里面的学问其实有很多。 服务的搭建（包括测试，灰度和生产环境的协调等细节） 服务器权限以及目录的管理（研发帐号，服务帐号，目录路径规范） 中间件的配置管理 系统参数优化以及中间件调优（主要是JVM调优，内核参数优化） 服务的监控（包括主机监控以及服务端口监控，告警通知等） 服务所使用的缓存管理 同时，从故障中我也领会到了不少门道。 部署环境（尤其生产）一定要考虑高可用 监控点必须覆盖全面（从主机到服务） 考虑备份服务（视情况而定） 尽量避免实施影响服务稳定性的需求（俗话说的不要给自己埋坑） （这里的总结只是本人的愚见，希望阅读的您能批评指正🙏 ） 开始探索前文说到的“深夜更新”以及故障处理确实挺艰苦的。为此，我开始思考怎么“偷懒”。😜首先是对付更新，我在想能不能自动化，让系统自己执行更新命令然后我睡大觉呢。😏然后是对付故障，大家都说防范于未然。那能不能将监控升级一下，监控不止于监控，还具备修复功能。于是，我开始探索实现的手段。接下来就记录几个栗子吧。😄]]></content>
      <categories>
        <category>个人感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase的一点小总结]]></title>
    <url>%2F2018%2F01%2F03%2Fmanage-hbase%2F</url>
    <content type="text"><![CDATA[前言记录一下当时在管理CDH版本的Hbase时候的一些小经验。 配置HBase CanaryCanary是一个阶段性检查RegionServer是否存活的服务。默认是关闭的，你可以启用这个服务，并设置一些相关属性。 启用Canary/usr/bin/hbase org.apache.hadoop.hbase.tool.Canary --daemon 具体可配置选项可以查看官网文档。 检查以及修复HBase表使用hbck命令有两种模式： 只检查不修复 检查并修复 运行hbck命令(不带参数：检查表并打印ok，若有问题，只会显示问题的数目。)/usr/lib/hbase/bin/hbase hbck 打印详细的检查日志：/usr/lib/hbase/bin/hbase hbck -details 检查指定的表： /usr/lib/hbase/bin/hbase hbck &lt;table1&gt; &lt;table2&gt; Hedged Read 如果正在读某个block比较慢，hdfs客户端会启动另一个并行的线程去读此block的副本。此功能只能在HFiles存储在hdfs上的时候被启用，默认是关闭的。启用Hedged Read在HBase集群的每一台机器上面的hbase-site.xml文件设置如下属性。 &lt;property&gt; &lt;name&gt;dfs.client.hedged.read.threadpool.size&lt;/name&gt; &lt;value&gt;20&lt;/value&gt; &lt;!-- 20 threads --&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.hedged.read.threshold.millis&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;!-- 10 milliseconds --&gt; &lt;/property&gt; 配置RS的数据均衡当时在HBase的管理界面上发现region在各个RS上面并不均衡，通过查阅文档得出需要配置以下两个属性。 &lt;property&gt; &lt;name&gt;hbase.balancer.period&lt;/name&gt; &lt;value&gt;3000000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.loadbalancer.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer&lt;/value&gt; &lt;/property&gt; 同时，如果对RS使用graceful_stop，这个脚本会去检查balancer是否开启，如果是，会自动把balancer关闭，故节点下线之后需要手动开启balancer。命令如下：balance_switch true 同时，也可以手动关闭balancer，命令如下；balance_switch false hbase节点平滑下线注：这是管理CDH版本Hbase的办法。 切换balance: balance_switch false unload: /usr/bin/hbase --config /etc/hbase/conf org.jruby.Main /usr/lib/hbase/bin/region_mover.rb unload &lt;hostname&gt; 停止RS: service hbase-regionserver stop 切换balance: balance_switch true 参考https://www.cloudera.com/documentation/enterprise/latest/topics/admin_hbase_config.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spark的一点小总结]]></title>
    <url>%2F2018%2F01%2F03%2Fmanage-spark%2F</url>
    <content type="text"><![CDATA[前言Spark应用类似于MR作业。在MR中，最高级的计算单元是作业。系统读取数据，map，shuffle，reduce，然后写回存储。在Spark中，最高级的计算单元是应用，应用可以运行一系列作业或者是并行作业。一个Spark的作业可以由多个阶段组成。Spark依赖driver进程管理工作流和安排任务。 Spark术语 RDD：Spark编程核心，是一个容错的元素集组成。可以并行地被多次处理。 partition：RDD的元素子集。partition是一个并行的单元，Spark可以并行处理多个partition的元素。 driver：一个app一个，负责初始化之类的工作。 executor：真正跑程序的进程。一个主机可能会有多个executor。 deploy mode：client mode的driver运行在cluster之外，cluster mode的driver运行在cluster内，并log out之后程序不会中断。 运行spark程序的方法可以使用spark-submit脚本提交已经编译好的spark程序。提交命令：spark-submit --option value application jar | python file [application arguments] 选项 描述 application jar 程序以及相关依赖的jar包 python file 程序的python文件 application arguments 程序需要的参数 optional table如下： 选项 描述 – – class 程序的主类，如org.apache.spark.examples.SparkPi – – conf spark的配置属性 – – deploy-mode cluster或者client，默认是client – – driver-cores driver使用的核心数，只在cluster模式生效，默认是1 – – jars 添加依赖的jar包，这些jar包需要在hdfs上，否则它们必须在每一个executor上 – – master 运行程序的地方 master值如下： master 描述 local 用一个worker线程（非并行） local[k] 用k个worker线程（k的值是主机的核心数） local[*] 用的worker线程和物理核心数一致 spark://host:post 在指定的主机上跑master进程 yarn 在yarn上运行 控制参数读取的优先顺序： SparkConf传入的参数 spark-submit，spark-shell或者pyspark传入的参数 spark-defaults.conf中设置的属性 集群模式概览下面是使用spark-submit提交一个应用到集群时所发生的事情： spark-submit启动driver进程并调用了应用的main方法。 driver进程向集群申请资源去启动executors 集群代替driver启动executor driver运行用户程序，将一系列操作发送到executors 任务运行 如果main方法里有exits或者调用SparkConf.stop，就会停止executors并释放资源 spark运行模式总结 模式 YARN Client Mode YARN Cluster Mode Spark Standalone Driver runs in Client ApplicationMaster Client Requests resources ApplicationMaster ApplicationMaster Client Starts executor processes YARN NodeManager YARN NodeManager Spark Worker Persistent services YARN ResourceManager and NodeManagers YARN ResourceManager and NodeManagers Spark Master and Workers Supports Spark Shell Yes No Yes Cluster Mode:Client Mode: 下面这些选项是用于提交Spark on YARN 应用程序： 选项 描述 – – executor-cores 分配给每个executor的核心数 – – executor-memory 每个executor的内存 – – num-executors 分配给这个spark应用的yarn container的总数 – – queue 提交到YARN队列名 Cluster mode的例子：spark-submit --master yarn --deploy-mode cluster SPARK_HOME/lib/spark-examples.jar --class org.apache.spark.examples.SparkPi 10 Client Mode的例子：spark-submit --master yarn --deploy-mode client SPARK_HOME/lib/spark-examples.jar --class org.apache.spark.examples.SparkPi 10 Spark动态分配executor如果程序资源需求经常变化，spark可以动态增加或者减少程序executor的数目。要使动态分配生效，可以设置spark.dynamicAllocation.enabled的值为true。程序分配最少的executor数目的属性是spark.dynamicAllocation.minExecutors，最多的是spark.dynamicAllocation.maxExecutors，初始化值是通过设置spark.dynamicAllocation.initialExecutors参数。这时候不要使用–num-executors参数或者设置spark.executor.instances这个参数，它们之间并不相容。 优化YARN模式通常，每次提交任务spark都会将assembly jar上传到hdfs，可以省下这步提高效率，可以手动上传之后设置SPARK_JAR环境变量。SPARK_JAR=hdfs://namenode:8020/user/spark/share/lib/spark-assembly.jar 参考https://www.cloudera.com/documentation/enterprise/latest/topics/admin_spark.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[管理YARN服务]]></title>
    <url>%2F2018%2F01%2F02%2Fmanage-yarn-2%2F</url>
    <content type="text"><![CDATA[前言关于YARN，在实际工作中我们做得最多的是集群调优，也就是将计算资源合理分配。本文将会给出资源分配的思路。 重温概念 RM：一个负责管理运行作业，分配AM到具体作业并且执行资源限制的主进程。 NM：一个负责启动AM和任务容器的工作进程。 AM：一个为执行任务申请资源的管理进程。 vcore：虚拟核心，一个逻辑的处理器单元。 container：任务的资源池。这里包括vcores和memory。 确定计算资源和服务需要首先需要确定每个计算节点拥有多少vcores，多少memory和多少个磁盘；其次，估计运行NM或者DN（或者该节点上需要运行的服务）需要多少资源，还需要减去操作系统本身的服务所需的资源。 估算资源CDH推荐有以下几点： 预留10-20%的内存给系统及其进程服务。 最少16GB的内存留给impalad进程。 不超过12-16GB的内存给RegionServer进程。 另外，需要预留资源给任务缓存，例如HDFS Sort I/O buffer。 对于vcore，考虑每个服务初始化时候的并发进程或者并发任务的数量。 举个栗子： 服务 vcore Memory(GB) 操作系统 1 8092 NodeManager 1 1024 DataNode 1 1024 Impala 0 0 RegionServer 0 0 Solr Server 0 0 available resource 45 251904 Total 48 262144 注：此表展示的为一个48C256G的计算节点资源分配，0为不运行在此节点。 NodeManager的分配 属性 默认值 解释 yarn.nodemanager.resource.cpu-vcores 8 可以分配给container的vcore数目 yarn.nodemanager.resource.memory-mb 8g 可以分配给container的内存总数(mb) 根据上面的栗子，假设我们有10个一样的worker节点，那么上面属性配置如下： key value yarn.nodemanager.resource.cpu-vcores 192(4倍超线程48*4) yarn.nodemanager.resource.memory-mb 251904mb Container的分配 key default value explain yarn.scheduler.minimum-allocation-vcores 1 一个container最小可申请的vcore数 yarn.scheduler.maximum-allocation-vcores 32 一个container最多可申请的vcore数 yarn.scheduler.increment-allocation-vcores 1 增长数 yarn.scheduler.minimum-allocation-mb 1024 一个container最小可申请内存 yarn.scheduler.maximum-allocation-mb 8192 一个container最大可申请内存 yarn.scheduler.increment-allocation-mb 512 增长数 上面属性可以根据实际情况的任务进行调节，调整完之后可以直接估算到集群同时能够跑多少个container。实际上我们生产环境是使用cgroup加上fair-scheduler来控制container的资源分配。 MapReduce的配置 key default value explain mapreduce.map.memory.mb 1GB 分配给一个job的每个map任务的物理内存总数 mapreduce.map.java.opts.max.heap 800MB map进程的最大java 堆大小（单位b） mapreduce.map.cpu.vcores 1 分配给一个job的每个map任务的虚拟核心数 mapreduce.reduce.memory.mb 1GB 分配给一个job的每个reduce任务的物理内存总数（MB） mapreduce.reduce.java.opts.max.heap 800MB reduce进程的最大java 堆大小（单位b） mapreduce.reduce.cpu.vcores 1 分配给一个job的每个reduce任务的虚拟核心数 yarn.app.mapreduce.am.resource.mb 1GB ApplicationMaster的物理内存需求（MB） yarn.app.mapreduce.am.command-opts 800MB AM的最大堆大小（单位b） yarn.app.mapreduce.am.resource.cpu-vcores 1 AM需要的虚拟核心数 属性 mapreduce.[map| reduce].memory.mb是指定container需要分配给这些map和reduce的内存，这些值应该设置成超出整个任务的堆大小Cloudera推荐的乘积因子是0.8。实际任务决定这个最优值。Cloudera同样推荐mapreduce.map.memory.mb的值是1-2GB并且设置 mapreduce.reduce.memory.mb的值是两倍的mapper值。如果实际jobs有很多并发的任务，需要增加AM的堆大小。 一点感想实际上当时我们的生产环境的资源控制层次是，底层使用cgroup控制，用户层面使用fair-scheduler进行分队列控制。具体配置方法可以参考我的部署脚本。 点击这里 参考https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_ig_yarn_tuning.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[管理HDFS服务]]></title>
    <url>%2F2018%2F01%2F01%2Fmange-hdfs%2F</url>
    <content type="text"><![CDATA[前言这篇文章主要记录一下我管理HDFS集群的时候遇到一些坑的填坑办法，用来提醒自己，也希望启迪他人。😄 NameNode管理失效的DataNode一般在一段时间（默认是10分钟）Namenode收不到DataNode的heartbeats就认为DN失效了。这时候：NN会查找哪些block在这个失效的DN上，并定位其他拥有这些block备份的DN，这些DN会将这些block复制到另外的DN上，以维持设置的复制因子数。同时将失效的DN下线。 配置DN的存储平衡当时遇到的一个坑是机器dn上面的磁盘大小不一致，然后首先是想到配置存储平衡看能不能让dn自己搞定。 存储平衡可以配置两个东西： DN内不同容量的磁盘允许多大的误差就被认为是平衡的 有多大比例的block会写到DN中容量比较大的那块磁盘里面。 需要配置的属性如下：dfs.datanode.fsdataset.volume.choosing.policy / org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy（使存储平衡生效）dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold / 10737418240 (default) （这个10G是指一个DN内磁盘最大和最小的容量差要在10G之内）dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction / 0.75(defalut) （百分之75的新blk会发送的容量更大的磁盘上）配置后重启DN即可。 dfs.datanode.du.reserved / 10GB 每一个dn的每一个目录保留的非hdfs空间。 后面发现平衡的效果并不是特别理想，不理想的原因其实是因为总可用的容量并不是hdfs report的那个总可用容量；下面做一个描述。看到上图，这个盘是243g，但是仔细一看，used+avail != size。。231g != 243g。。再看一下hdfs report出来的结果：一看好像没啥特别，但是仔细一观察，DFS Remaining怎么就比磁盘的avail还要大呢。。这是要写爆盘的节奏吗。。。其实这里的原理是，Configured Capacity是等于磁盘总容量减去我们上面配置的那个du.reserved的值，也就是243-10，就是233左右，然后hdfs认为已经给你预留了，剩下的都可以写数据了。但是，实际上是不行的。。因为这个磁盘本来可用就少了12g。所以就会出现你信心满满的配置好了上面的属性之后，仍然会写爆磁盘的情况。。解决办法是将du.reserved的值设置大一点，令到hdfs做完减法之后DFS Remaining要少于avail就好了。但是对于追求稳定完美的运维来说，好像还是不那么舒服，因为总是感觉一边大一边小，资源利用率不是那么高。于是我们就想着把磁盘都搞成一样大的。通过讨论，我们做了如下操作： 一个dn上磁盘大小不一致的时候， 可以将数据目录移到大的磁盘上，然后做一个软连接到原来的目录上。将软连接写到配置文件中即可。当然这是在有数据的时候。也可以使用将dn下线的办法再更换数据目录。软连接的权限不影响，实体目录的权限才需要配置正确. 这样我们就可以把小的磁盘去掉，留一个软连接就好了。 配置本地读（Short-circuit read）配置如下： 在hdfs-site.xml文件中添加： &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.size&lt;/name&gt; &lt;value&gt;1000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.read.shortcircuit.streams.cache.expiry.ms&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.domain.socket.path&lt;/name&gt; &lt;value&gt;/var/run/hadoop-hdfs/dn&lt;/value&gt; &lt;/property&gt; HDFS balancersbalancer会移动block和data直到每一个DN的用量和集群的用量不超过一个给定的阈值。（每一个DN的用量指该节点中使用的空间占整个节点的空间的百分比，而集群的用量是指整个集群的使用百分比）。balancer并不会平衡一个DN内部不同磁盘的容量。（比如一个DN中有两个目录数据量不一样，但是balancer不会令到两个目录数据平均分）。运行balancer的命令： sudo -u hdfs hdfs balancer （不加任何参数的情况下默认的threshold为10%，这个10%的意思是比如整个集群的用量是40%，balancer会保证每个DN的磁盘用量会在30%和50%（占那个DN的总量）之间。） sudo -u hdfs hdfs balancer -threshold 5（5%）同时还可以调整balancer的网络带宽： hdfs dfsadmin -setBalancerBandwidth &#60;newbandwidth&#62; 配置NFS gatewayNFSv3 gateway允许客户端将 HDFS mount到本地文件系统。这个gateway可以是集群中的任意一台机器。安装配置步骤： 安装所需的软件包： sudo yum install nfs-utils nfs-utils-lib hadoop-hdfs-nfs3 rpcbind* 在NN上的hdfs-site.xml中配置如下属性： &lt;property&gt; &lt;name&gt;dfs.namenode.accesstime.precision&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;/property&gt; 在NFS server中的hdfs-site.xml添加： &lt;property&gt; &lt;name&gt;dfs.nfs3.dump.dir&lt;/name&gt; &lt;value&gt;/tmp/.hdfs-nfs&lt;/value&gt; &lt;/property&gt; 在NN的core-site.xml中添加代理用户： &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hdfs.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.hdfs.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; 重启NN 在NFS服务器上： sudo service nfs stopsudo service hadoop-hdfs-nfs3 start 检查是否正常工作： rpcinfo -p &lt;nfs_server_ip_address&gt;; 验证hdfs是否可以被mount，使用showmount命令： showmount -e &lt;nfs_server_ip_address&gt;; 挂载hdfs： mount -t nfs -o vers=3,proto=tcp,nolock，noatime &lt;nfs_server_hostname&gt;:/ /&lt;hdfs_nfs_mount&gt; 这个服务只识别UID和GID，如果不同机器的用户UID和GID不同，mount上去之后，文件在不同机器便会显示属于不用的用户 如果使用了kerberos认证，则需要在nfs server的hdfs-site.xml文件里面加上以下属性： &lt;property&gt; &lt;name&gt;dfs.nfs.keytab.file&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf/hdfs.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.nfs.kerberos.principal&lt;/name&gt; &lt;value&gt;hdfs/_HOST@YOUR-REALM.COM&lt;/value&gt; &lt;/property&gt; 设置HDFS的Quotashdfs quotas有以下两种形式： 限制某个目录下的文件数量 限制某个目录的空间大小 上面两种形式是独立的，文件或者目录的创建如果会引起quota超出则会失败。 启用hdfs space quotas：hdfs dfsadmin -setSpaceQuota n dir （上面n是字节数，dir是需要设置限额的目录。后面可以接多个目录，n则会对这多个目录生效。）取消hdfs space quotas：hdfs dfsadmin -clrSpaceQuota dir （dir 是需要设置的目录，后面可接多个） 启用hdfs name quotas：hdfs dfsadmin -setQuota n dir （n代表目录和文件数量，dir是需要设置的目录，后面可接多个，n则会对多个生效。比如n是1，则不能往目录放任何东西，1代表目录本身，要从2开始）取消hdfs name quotas：hdfs dfsadmin -clrQuotas dir （dir是需要设置的目录，后面可接多个） 配置可挂载的HDFS 安装hadoop-hdfs-fuse： sudo yum install hadoop-hdfs-fuse 创建挂载点： mkdir -p &lt;mount_point&gt;; hadoop-fuse-dfs dfs://&lt;nameservice_id&gt;; &lt;mount_point&gt; 卸载挂载点： umount &lt;mount_point&gt; 配置中央缓存管理这是一个精确的缓存机制，它允许用户缓存hdfs上某一个路径下的文件。可以防止常用的数据被清出内存，NN管理缓存便于作业的安排与调度，同时可以提高整个集群的内存利用率（通过指定某些副本，从而免去全部副本缓存buffer的开销）。在上面的架构中，NN负责协调集群中所有DN的内存。NN会定期收到来自集群中的每一个DN描述各自块缓存的情况。NN管理DN缓存是通过使用捎带确认标志的心跳信号传送缓存或者接收缓存命令。NN通过查看它自身的缓存指令集去决定什么文件或目录应该被缓存。缓存指令被持久存储在FsImage和EditLog上面，并且可以使用命令行或者java接口去增删改。NN同时也保存了一系列缓存池，用来分类管理属于资源管理的缓存命令和执行权限的缓存命令。NN会定期重复扫描命名空间和现有的缓存目录去决定哪些块需要被缓存或者解除缓存并分配缓存到DN。用户增加或者删除缓存指令或者删除缓存池同样会触发重复扫描。缓存指令定义了一个需要被缓存的路径。路径可以是文件或者目录。目录并不会被循环缓存，也即是只有第一层目录的文件会被缓存。缓存指令的参数有诸如缓存复制因子和过期时间。这里的复制因子表示blk需要被缓存的副本数。如果出现多个缓存指令指向同一个文件，则会取最大的复制因子数。缓存池是用于将缓存指令分组管理。缓存池的权限管理就类似于UNIX。写权限允许用户增加和删除池中的缓存指令。读权限允许用户列出池中的缓存指令和一些元数据。执行权限未被使用。缓存池同样被用来作资源管理。缓存池可以限制池中所有缓存指令可缓存的字节数和最大的过期时间。 缓存指令的命令： 新增：hdfs cacheadmin -addDirective -path &#60;path&#62; -pool &#60;pool-name&#62; [-force] [-replication &#60;replication&#62;] [-ttl &#60;time-to-live&#62;]（path指需要被缓存的目录或者文件的路径，pool-name指路径加到那个池中，这里需要拥有写权限。force指跳过检查缓存池的资源限制。replication指复制因子，默认是1。time-to-live指缓存生效时间。可以使用秒，分，时或者日作为单位，例如：30m,4h,20d。never则是永不过期。默认是never） 删除：hdfs cacheadmin -removeDirective &#60;id&#62;（id指需要被删除的缓存指令的id，可以使用list查看，需要有写权限） hdfs cacheadmin -removeDirectives &#60;path&#62; （path指路径中的所有缓存指令，需要写权限） 列举：hdfs cacheadmin -listDirectives [-stats] [-path &#60;path&#62;] [-pool &#60;pool&#62;] （path指该路径的缓存指令，需要读权限。pool指该池中属于那个路径的指令。stats指统计信息） 缓存池命令： 新增：hdfs cacheadmin -addPool &#60;name&#62; [-owner &#60;owner&#62;] [-group &#60;group&#62;] [-mode &#60;mode&#62;] [-limit &#60;limit&#62;] [-maxTtl &#60;maxTtl&#62;] （name指池的名字。owner可指定，默认是当前用户。group指池的分组。mode指权限，默认是0755。limit指池中所有指令总共可以缓存的字节数，默认是无限制。maxTtl指池中指令最大的过期时间，默认没有最大值） 修改：hdfs cacheadmin -modifyPool &#60;name&#62; [-owner &#60;owner&#62;] [-group &#60;group&#62;] [-mode &#60;mode&#62;] [-limit &#60;limit&#62;] [-maxTtl &#60;maxTtl&#62;] 删除：hdfs cacheadmin -removePool &#60;name&#62; 列举：hdfs cacheadmin -listPools [-stats] [&#60;name&#62;] （stats指统计信息） 配置缓存： libhadoop.so 设置hdfs-site.xml里面的dfs.datanode.max.locked.memory属性（需要考虑为DN，jvm heap和操作系统的页缓存预留内存） 调整ulimit -l 出来的值必须要比上面设置的属性值要大或者值是没有限制。（需要注意的是，上面属性的value的单位是b，而ulimit -l的单位是KB） 参考https://www.cloudera.com/documentation/enterprise/latest/topics/admin_hdfs_config.html]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Ansible部署CDH]]></title>
    <url>%2F2017%2F12%2F30%2Fansible-deploy-cdh%2F</url>
    <content type="text"><![CDATA[部署准备本次部署采用ansible，以及CDH(cloudera hadoop)。由于作为例子，仅展示最小化的集群部署（六个节点）。详细部署方式可以参考官方文档。 点击这里 角色分配cdh-test-01: nn, rm, hmastercdh-test-02: nn, rm, hmastercdh-test-03: dn, nm, zk, jn, rscdh-test-04: dn, nm, zk, jn, rscdh-test-05: dn, nm, zk, jn, rscdh-test-client: hive, spark, hue, jhs-httpfs, hbase-thriftserver, mysql 注：我感觉读这篇文章的你肯定能猜到上面的缩写是什么呢。😏 部署细节我使用ansible部署这套cdh集群的时候，遇到的难题主要是如何令到整个部署过程更加顺滑。这里面其实就是ansible的使用哲学，主机组的定义以及role的编排。这里面我目前想到的是两种编写ansible脚本的方式。一种是一个组件一个role一个对应的主机组，比如，hdfs这一个组件写成一个role，主机组里面写hdfs需要的全部主机，然后role里面的task就需要写得比较复杂一点，因为不同主机安装不同的包和下发不同的配置文件；另一种是以主机角色区分role，主机组也是以主机角色进行编排。比如，前面两台机cdh-test-0[1,2]我定义在了hadoop-master和hbase-master的组里面，cdh-test-0[3,4,5]我定义在了hadoop-slave和hbase-slave的组里面，分别对应不同的role，role里面的task就可以写得比较简单，只需要关注某一个角色的安装包和配置文件即可。同时，每个组的配置文件所需要的变量都配置在该组的group_vars里面，这样做可以令到部署的条理较为清晰，便于日后维护。 除了以上还需要记录几点： 使用了kerberos提高集群的安全性，访问数据以及提交任务都需要使用对应用户的keytab，需搭建krb认证服务和dns服务,生成用户keytab和服务keytab hive的metastore使用的是remote方式，暂时将mysql装在client节点上，是单节点，生产环境需要搭建主从可切换 集群的底层资源控制使用的是cgroup，cgroup可以更好地将主机资源进行有效分配，任务层面的资源调度使用的是fair-scheduler 配置文件里面涉及到帐号密码等敏感信息建议使用ansible-valut来进行加密 安装脚本https://github.com/LEUNGUU/ansible-cdh欢迎批评指正，谢谢！😄]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hbase架构]]></title>
    <url>%2F2017%2F12%2F30%2FHbase-architecture%2F</url>
    <content type="text"><![CDATA[在介绍hbase主要的角色HMaster和Regionserver的时候，我们先来看看元数据表hbase:meta。在0.96之前，元数据表是分成-ROOT-和.META.两张表的，0.96之后统一变成hbase:meta了。我们主要介绍一下hbase:meta。 hbase:metahbase:meta是一张保存了所有region信息的储存在zookeeper里面的表。表结构如下： KeyRegion key of the format ([table],[region start key],[region id]) Valuesinfo:regioninfo (serialized HRegionInfo instance for this region)info:server (server:port of the RegionServer containing this region)info:serverstartcode (start-time of the RegionServer process containing this region) 以下几点值得注意： 当一个region正在分裂的时候，会产生另外两列，分别是info:splitA和info:splitB，当region完成分裂之后，这两列就会被删除。 HRegionInfo是用来表示table的开始和结束。如果一个region有一个empty start key则表示它是这个表的第一个region；如果一个region同时有start key和end key则表示这张表只有一个region。 启动的时候，hbase会先去zookeeper查找hbase:meta这张表，然后更新表信息。具体是master启动AssignmentManager，AM在meta表里查找region信息（如果RegionServer(RS)健在，那么region分配位置继续保持；如果RS不在了，LoadBalancerFactory会被启动重新分配region到新的RS）。然后meta表会更新两个信息，一个是region新的分配位置，另一个是这个RS开始处理这个region的时间。 HMasterHMaster负责监控集群里面所有的RS实例和更新meta表。通常运行在NameNode节点上。HMaster周期性运行以下两种线程： LoadBalancer：用来平衡集群内的Region CatalogJanitor：检查和清理meta表 RegionServerRS负责管理Regions。通常运行在DataNode节点上。RS主要运行以下几种线程： CompactSplitThread：检查split和管理compactions MajorCompactionChecker：检查主要的compactions MemStoreFlusher：周期性将MemStore写入StoreFiles LogRoller：周期性检查RS的write ahead log Region分裂过程RS负责客户端的写操作，将数据先写进内存的一个对象memstore。一旦memstore满了，随即就会将内容写到磁盘的storefiles，这个过程叫memstore flush。一旦storefiles多起来了，RS便会将它们合并到一起形成更大的storefiles，这个过程叫compact。这个过程不断重复，region会越来越大。这时候，RS就会根据策略判断是否需要将region分裂(split)。我们来看下图： 详细过程： RS一旦决定要将region进行分裂，那么分裂过程就开始了。RS会申请一个共享读锁来防止表的修改，然后在zk的/hbase/region-in-transition/region-name目录下面创建一个znode，状态是SPLITTING HMaster通过监控znode(region-in-transition)立马就知道这个过程开始了 RS在HDFS里面的region目录下新建一个.splits子目录 RS通过更改本地的数据层次将这个即将被分裂的region标记为offline。这是如果有客户端对这个region进行请求将会收到NotServingRegionException，客户端将会尝试其他region RS在.split这个目录里面创建两个子目录分别给daughter region A&amp;B，同时创建必要的数据结构，然后开始分裂那些storefiles。总的来说，这时候每个storefile都会有两个reference file被创建，这些refernce file分别根据分裂的位置（split key ponit）指向父region的不同位置 RS在HDFS目录里面创建新的region目录并将reference files移动到每一个新的region目录下 RS请求更改meta表，信息具体如图。 RS并行地启用daughter A&amp;B RS将新的ABregion加到meta表并设置状态为online RS更新znode(/hbase/region-in-transition/region-name)状态为SPLIT。然后master便可以通过zk知道这个分裂过程已经完成 这时候references依然存在，但是当有新数据写入触发compact过程的时候，这些references将会被移除。Master的垃圾回收机制会周期性去检查这些新的region是否仍然有指向父region(reference files)，如果没有，父region将会被移除。 以上所描述的是大致的过程，具体详细的实现建议直接阅读hbase的源码。 参考http://archive.cloudera.com/cdh5/cdh/5/hbase/book.html#_architecture]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hive架构]]></title>
    <url>%2F2017%2F12%2F28%2FHive-architecture%2F</url>
    <content type="text"><![CDATA[下图展示了Hive的组成以及Hive和Hadoop的交互。 Hive组件UI： 用户提交查询语句以及其它管理操作的入口(command line) Driver： 接收用户提交的查询语句，与Compiler创建会话连接。 Compiler： 对查询语句进行语义分析并生成执行计划 Metastore: 元数据库，存储表的结构信息，分区信息以及与HDFS文件的关联信息。 Execution engine: 执行Compiler生成的执行计划（管理不同执行阶段的执行依赖以及在合适的系统组件上执行这些阶段任务） Hive 查询语句的执行过程看图说话。UI将查询语句传给Driver，Driver为查询语句创建一个session并将语句传给Compiler，Compiler去元数据库获取元数据用来分解查询语句并生成这些语句的执行计划返回给Driver，Driver叫Execution engine执行Compiler生成的任务，Execution engine调用Hadoop的各个组件执行相应的任务，然后获取返回结果。 Hive数据模型Hive里面主要有三种数据结构： Tables这和关系型数据库的表类似。表数据存储于HDFS的一个目录。Hvie支持外部表的创建，将HDFS目录里的数据位置写到DDL语句里面即可。 Partitions每张表都可以根据一个列来进行分区，表数据会根据分区来进行分目录存储，提高查询效率。 Buckets每个分区的数据都根据列的哈希值来分成几个Buckets，用以提高查询效率。 除了原始的列类型(integers, floating point numbers, generic strings, dates and booleans)之外，hive同样支持array和maps，还支持用户自定义类型。这令到hive存储的数据类型的范围更宽广。 元数据对象 Database：表的命名空间。其实就是用来组织不同的表。 Table：表的元数据包含列信息，表的拥有者，存储信息以及序列化信息。 Partition：每一个分区都有它的列，存储和序列化信息。 元数据库架构 内嵌：使用Derby作为metastore，client直接连接即可。 远程：推荐使用mysql，这时候metasore是一个Thrift的服务。mysql可以做主从架构。 参考https://cwiki.apache.org/confluence/display/Hive/Home]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS和YARN的架构]]></title>
    <url>%2F2017%2F12%2F25%2FHDFS%E5%92%8CYARN%E7%9A%84%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[前言&emsp;&emsp;在企业里面，我们搭建一个服务一般都会考虑几个问题。首先是用途，比方说一般都有开发环境，测试环境以及生产环境（比较重要的服务一般还会有灰度环境）。这个用途一旦决定了，我们就可以考虑接下来的问题了。第二是架构，第三就是成本了。架构会直接决定所需要花费的机器成本和维护成本。好吧，好像有点扯远了。。。回到正题。这里我想要说明的事情是我当上大数据运维之后，遇到的坑其实都可以归根溯源到架构或者原理的理解上面。所以我特别想记录一下hdfs和yarn的架构，遇到坑的时候回想一下或许可以从这里找到解决问题的思路。 HDFShdfs包含两种角色，Namenode和Datanode。下面是一张官方架构图。&emsp;&emsp;hdfs是典型的主从架构。在hdfs的内部，文件是会被划分成多个块，块被存储在一组slave里面（就是图里的绿色小方格）。&emsp;&emsp;master负责管理文件系统的命名空间（比如打开文件，关闭文件以及重命名文件和文件夹）,客户端对文件系统的访问权限以及块和slave的映射关系。&emsp;&emsp;slave负责存储文件，处理客户端的读写请求并且根据master的指令对块进行创建，删除以及复制。 下面分几个方面描述一个hdfs这个分布式文件系统。 命名空间 像大多数文件系统一样，用户可以对文件（夹）进行增删改查。NN负责维护这些对命名空间进行操作的元数据信息。 数据复制 在HDFS的内部，文件会被分成大小相同的多个块进行存储，同时，为了实现容错，这些块会被复制多份并散落在不同的DN上进行存储。这个块大小和复制份数是可以由用户定义的。上面提到DN是存储这些块的，NN是负责管理块和DN的映射关系，那么，这里我们有两个东西，一个是心跳，另一个是块报告。DN周期性向NN发送心跳信息和块报告。心跳是告诉NN它还活着，块报告是告诉NN它有哪些块。详细如图所示。 那么，块是怎么存储会比较方便呢？这里面我们考虑两个东西。一个是高可用，另一个是读写性能。上面提到了，为了保证数据安全可靠，hdfs会将数据分割成多个大小相同的块并且对块进行复制之后分散存储。虽然这个做法满足了数据安全可靠的要求，但是此时我们遇到了另一个问题，读写效率。如果我们将块存储在不同机架，带宽会成为效率的瓶颈。但是都存在一个机架，那么安全可靠性就大大降低了。通常的做法是将复制因子设置为3，然后按照以下策略放置副本。 副本No.1：如果客户端正在这个DN上面写，那么就放在这个DN；否则，在这个机架任意选取一个DN 副本No.2：放在和第一份副本不同的机架上面的某个节点 副本No.3：放在第二份副本那个机架上的不同节点上面 总的来说，就是一个副本在一个节点，另一个副本在另一个机架的一个节点上，最后一个副本均匀分布在第二个副本那个机架的不同节点上。使用这个策略我们提高了读写效率（降低机架之间传输的所需带宽）和不影响数据的安全可靠。同时，HDFS有一个就近读的策略，默认是会选取离接收读请求那个DN最近的块以降低带宽的开销。这是所谓的块选择。同时，这里有一个安全模式，当NN发现它安全的时候，它才会继续接受请求。那怎么判断安全呢？那就是副本数达到一个可承受的最小值。 元数据持久化 这里我们说两个文件，一个是EditlLog，另一个是FsImage。 EditLog：记录文件系统元数据的变化。比如新增一个文件之类。修改复制因子的操作这里也会记录 FsImage：保存整个文件系统的命名空间，包括文件的块映射以及文件系统的一些属性 其实EditLog是记录操作的，FsImage是保存被操作的文件系统的。那么这里我们有一个叫checkpoint的东西，它的工作原理是当NN启动的时候，会从磁盘读取EditLog和FsImage这两个文件，并将EditLog里面记录的操作应用到FsImage然后生成一个新版本的FsImage，然后会删掉这个EditLog。这个过程就是checkpoint了。 鲁棒性 磁盘故障，心跳信息与重复制每个DN都会定期发送心跳信息到NN表示自己还活着。假设出现网络问题，NN会标记此DN已经失联，这时候便不再转发任何的IO请求给它，同时NN会触发re-replication。其实，磁盘只读，DN失联或者block损坏都是会令NN触发re-replication的。 数据检验NN会对新写入的文件做一个校验和然后存在一个隐藏文件里面，当客户端要读这个文件，NN会将从各个DN读回来的block的检验值和隐藏文件里面的进行对比。如果对，那么读出来，错误，就继续寻找其他无损的block。 snapshot其实就是用来回滚到你做这个快照的时候的状态。 数据写过程 数据块HDFS天生就是用来处理大文件的。默认的块大小是64mb，可以在配置文件调节。 写过程写入数据的时候客户端会现将内容写到一个临时文件里，当临时文件超过块大小之后，客户端会通知NN。NN会修改文件系统层级并为其分配一个数据块，NN响应客户端的请求，告诉客户端放到哪个DN的哪个数据块里面去。然后客户端将临时文件的内容写到NN指定的DN。当一个文件写完了，剩下的临时文件会直接被发送到DN，然后客户端告诉NN搞定了。NN确认之后将文件状态从创建变为永久存储。（如果文件创建过程中NN故障了，那么文件就丢失了。） 复制是管道式的正如上面所说，写的过程里面客户端会获取一个可写的DN列表。这时候（假设复制因子是3），客户端将数据写到第一个DN。第一个DN接收数据并写到本地，然后将数据传输到列表里面的第二个DN，第二个DN和第三个DN同理。 空间回收 文件删除与恢复文件被删之后并不是马上移除，而是被移动到trash目录，这时候是可以被恢复的。用户可以设置一个过期时间，时间一过，文件就会被NN移除了。文件移除与空间释放之间会有一个延时。 YARN&emsp;&emsp;yarn有几个角色，都比较重要。我们先看图然后再总结一下角色的作用。 ResourceManagerRM主要有两个组件，一个是scheduler，另一个是ApplicationManager。 scheduler这个调度器主要作用是分配资源给任务，比如CPU，memory等等。最流行的有两种调度策略，分别是CapacityScheduler 和 FairScheduler。 ApplicationManager这个应用管家主要负责接受任务提交，为任务启动ApplicationMaster以及负责ApplicationMaster失败后的重启。 NodeManager如其名，这个组件负责监控并且向RM汇报资源的使用情况。 ApplicationMaster每个任务的AM负责根据调度器的安排分配资源细节，跟踪状态并且监控任务过程。 其实，我们对着上面的这个框架图然后知道每个角色的作用就可以明白YARN架构的工作流程了。 好了，到这里的话我大致就讲完hadoop这个东东里面的主要部分了。我当时所在的团队使用的是CDH版本的hadoop，和社区版还是有一点区别的。但是里面的原理是一样的。 参考http://archive.cloudera.com/cdh5/cdh/5/hadoop/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回忆录（大数据篇）]]></title>
    <url>%2F2017%2F12%2F24%2F%E5%9B%9E%E5%BF%86%E5%BD%95%EF%BC%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%96%B9%E5%90%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[互联网生涯开始了刚毕业的时候我在一家外包公司，驻点在家乡的移动公司工作。每天除了做excel表格和催工程单之外，我找不到一点做技术的感觉。但是当时的我对大数据可是满怀热情，通过网络课程等途径自学了许多大数据的知识。还记得当时经常上dataguru论坛，听dataguru的课程，听tiger老师讲授hadoop的各种知识。 初识hadoop对hadoop生态圈形成概念并开始动手操作是在听dataguru的tiger老师课程的时候。那时候很认真听每一节课，从hadoop的原理到动手搭建伪分布式集群，都认认真真一步一步按照老师的讲法来操作。这个过程中也算是开始熟悉了shell命令，为之后的运维工作打下了一个基础吧，同时还有一个东西我要推荐的，就是Packt这个出版社。它真的是技术人的福音！它出版的书籍可操作性强，条理清晰，贴近前沿。 大数据运维突然由于项目被取消了，我毕业后的第一份工作也结束了。其实当时的我对于被遣散其实是有一点点喜悦的，因为感觉终于可以离开这个不适合我的地方了。我觉得凭借着我自学的知识以及对大数据的热情，我肯定可以找到一个可以让我大展身手的地方。终于，经过努力，我通过面试进了a公司，岗位正是大数据运维。 社区版和企业版到了a公司之后发现其实自己之前学到的东西只是冰山一角。真正的企业里面使用的大数据集群要求要比我学到的那些要高出很多。a公司也算是一个中型互联网公司，大数据集群也有过百节点。不过，自学的东西也算是一个基础，正是有了这个基础，我才可以更好更快地把公司的那套集群掌握。接下来我将会讲述我在这整个过程里面所学到的东西。 HDFS和YARN的架构 Hbase架构 Hive架构 好了，架构就暂时说到这里，我们看看一些安装经验。之前在a公司的时候我们大数据运维组采用了两种部署方式，第一种是利用saltstack，另一种是利用ansible。ansible的方式是我写的，所以在这里我说一下ansible如何自动化部署CDH集群。 ansible部署CDH 除了部署之外，日常最多的工作就是维护集群稳定，并且实现来自产品，研发等的需求。现在分享一下日常的一些维护小经验。 管理HDFS服务 管理YARN服务 Hbase的一点小总结 Spark的一点小总结]]></content>
      <categories>
        <category>个人感悟</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[关于HTTP状态码的总结]]></title>
    <url>%2F2017%2F12%2F23%2F%E5%85%B3%E4%BA%8EHTTP%E7%8A%B6%E6%80%81%E7%A0%81%E7%9A%84%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[HTTP状态码的大致分类 100-199 用于指定客户端相应的某些动作。 200-299 用于表示请求成功。 300-399 用于已移动的文件并常被包含在定位头信息中指定新的地址信息。 400-499 用于指出客户端的错误。 500-599 用于支持服务器错误。 100-199 100 (Continue/继续)如果服务器收到的header中带有100-continue的请求，这是指客户端询问是否可以在后续的请求中发送附件。在这种情况下，服务器用100允许客户端继续或用417 (Expectation Failed)告诉客户端不同意接受附件。 101 (Switching Protocols/转换协议)101是指服务器将按照其上的头信息变为一个不同的协议。 200-299 200 (OK/正常)200的意思是一切正常。一般用于相应GET和POST请求。 201 (Created/已创建)201表示服务器在请求的响应中建立了新文档；应在header中给出它的URL。 202 (Accepted/接受)202告诉客户端请求正在被执行，但还没有处理完。 203 (Non-Authoritative Information/非官方信息)203表示文档被正常的返回，但是由于正在使用的是文档副本所以某些响应头信息可能不正确。 204 (No Content/无内容)204表示在并没有新文档的情况下，204确保浏览器继续显示先前的文档。这个状态码对于用户周期性的重载某一页非常有用，并且你可以确定先前的页面是否已经更新。 205 (Reset Content/重置内容)205重置内容的意思是虽然没有新文档但浏览器要重置文档显示。这个状态码用于强迫浏览器清除表单域。 206 (Partial Content/局部内容)206是在服务器完成了一个包含Range头信息的局部请求时被发送的。 300-399 300 (Multiple Choices/多重选择)300表示被请求的文档可以在多个地方找到，并将在返回的文档中列出来。如果服务器有首选设置，首选项将会被列于定位响应头信息中。 301 (Moved Permanently)301这个状态是指所请求的文档在别的地方；文档新的URL会在定位响应头信息中给出。浏览器会自动连接到新的URL。 302 (Found/找到)302与301有些类似，只是定位头信息中所给的URL应被理解为临时交换地址而不是永久的。 303 (See Other/参见其他信息)303这个状态码和 301、302 相似，只是如果最初的请求是 POST，那么新文档（在定位头信息中给出）要用 GET 找回。 304 (Not Modified/为修正)304表示当客户端有一个缓存的文档，通过提供一个 If-Modified-Since 头信息可指出客户端只希望文档在指定日期之后有所修改时才会重载此文档，用这种方式可以进行有条件的请求。 305 (Use Proxy/使用代理)305表示所请求的文档要通过定位头信息中的代理服务器获得。 307 (Temporary Redirect/临时重定向)浏览器处理307状态的规则与302相同。307状态被加入到 HTTP 1.1中是由于许多浏览器在收到302响应时即使是原始消息为POST的情况下仍然执行了错误的转向。只有在收到303响应时才假定浏览器会在POST请求时重定向。添加这个新的状态码的目的很明确：在响应为303时按照GET和POST请求转向；而在307响应时则按照GET请求转向而不是POST请求。 400-499 400 (Bad Request/错误请求)400指出客户端请求中的语法错误。 401 (Unauthorized/未授权)401表示客户端在授权头信息中没有有效的身份信息时访问受到密码保护的页面。这个响应必须包含一个WWW-Authenticate的授权信息头。 403 (Forbidden/禁止)403表示除非拥有授权否则服务器拒绝提供所请求的资源。这个状态经常会由于服务器上的损坏文件或目录许可而引起。 404 (Not Found/未找到)404状态每个网络程序员可能都遇到过，他告诉客户端所给的地址无法找到任何资源。 405 (Method Not Allowed/方法未允许)405指出请求方法(GET, POST, HEAD, PUT, DELETE, 等)对某些特定的资源不允许使用。 406 (Not Acceptable/无法访问)406表示请求资源的MIME类型与客户端中Accept头信息中指定的类型不一致。 407 (Proxy Authentication Required/代理服务器认证要求)407与401状态有些相似，只是这个状态用于代理服务器。该状态指出客户端必须通过代理服务器的认证。代理服务器返回一个Proxy-Authenticate响应头信息给客户端，这会引起客户端使用带有Proxy-Authorization请求的头信息重新连接。 408 (Request Timeout/请求超时)408是指服务端等待客户端发送请求的时间过长。 409 (Conflict/冲突)该状态通常与PUT请求一同使用，409状态常被用于试图上传版本不正确的文件时。 410 (Gone/已经不存在)410告诉客户端所请求的文档已经不存在并且没有更新的地址。410状态不同于404，410是在指导文档已被移走的情况下使用，而404则用于未知原因的无法访问。 411 (Length Required/需要数据长度)411表示服务器不能处理请求（假设为带有附件的POST请求），除非客户端发送Content-Length头信息指出发送给服务器的数据的大小。 412 (Precondition Failed/先决条件错误)412状态指出请求头信息中的某些先决条件是错误的。 413 (Request Entity Too Large/请求实体过大)413告诉客户端现在所请求的文档比服务器现在想要处理的要大。如果服务器认为能够过一段时间处理，则会包含一个Retry-After的响应头信息。 414 (Request URI Too Long/请求URI过长)414状态用于在URI过长的情况时。这里所指的“URI”是指URL中主机、域名及端口号之后的内容。 415 (Unsupported Media Type/不支持的媒体格式)415意味着请求所带的附件的格式类型服务器不知道如何处理。 416 (Requested Range Not Satisfiable/请求范围无法满足)416表示客户端包含了一个服务器无法满足的Range头信息的请求。 417 (Expectation Failed/期望失败)如果服务器得到一个带有100-continue值的Expect请求头信息，这是指客户端正在询问是否可以在后面的请求中发送附件。在这种情况下，服务器也会用该状态告诉浏览器服务器不接收该附件或用100状态告诉客户端可以继续发送附件。 500-599 500 (Internal Server Error/内部服务器错误)500是常用的“服务器错误”状态。该状态经常由服务端程序引起也可能由无法正常运行的或返回头信息格式不正确引起。 501 (Not Implemented/未实现)501状态告诉客户端服务器不支持请求中要求的功能。例如，客户端执行了如PUT这样的服务器并不支持的命令。 502 (Bad Gateway/错误的网关)502被用于充当代理或网关的服务器；该状态指出接收服务器接收到远端服务器的错误响应。 503 (Service Unavailable/服务无法获得)503表示服务器由于在维护或已经超载而无法响应。服务器可提供一个Retry-After头信息告诉客户端什么时候可以在试一次。 504 (Gateway Timeout/网关超时)该状态也用于充当代理或网关的服务器；它指出接收服务器没有从远端服务器得到及时的响应。 505 (HTTP Version Not Supported/不支持的 HTTP 版本)505表示服务器并不支持在请求中所标明 HTTP 版本。 注： 本文仅作为个人总结参考。]]></content>
      <categories>
        <category>网络</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[写在开始]]></title>
    <url>%2F2017%2F12%2F22%2F%E5%86%99%E5%9C%A8%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[悟以往之不谏，知来者之可追。识迷途其未远，觉今是而昨非。 ——陶渊明 本主页是博主用来记录工作和生活中的点滴，希望像古人说的那样时常回顾过去，总结自身，展望未来。笔者至今工作三年有余，从一个什么都不懂的小白，到苦逼的业务运维，再到会“偷懒”的运维开发，中间踩过不少坑，也填过不少坑。希望在这里记录一下成长的历程。用以提醒自己，同时希望可以启迪读者。 经过思考，将会做成回忆录的形式。脑海里始终萦绕着一句话，“一段往事，一种技术”。既是回忆往事经历，同时也是回忆所学之技术。或许，这就是我写这个博客的主要目的。 最后，借用曾子的一句话来勉励一下自己， “吾日三省吾身——为人谋而不忠乎？与朋友交而不信乎？传不习乎？”]]></content>
      <categories>
        <category>个人感悟</category>
      </categories>
  </entry>
</search>
